{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc32440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search queries (comma-separated): Mtn\n",
      "Enter year to search from: 2023\n",
      "Enter title for scraped data set: mtn_2023\n",
      "Searching for tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=Mtn+since%3A2023-01-01+until%3A2023-12-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=Mtn+since%3A2023-01-01+until%3A2023-12-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=Mtn+since%3A2023-01-01+until%3A2023-12-31&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "\n",
      "Here are the first 10 tweets:\n",
      "\n",
      "Thank you for using my tool! Open mtn_2023.csv to view your scraped data.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import re\n",
    "\n",
    "# prompt user for search queries and year to search from\n",
    "search_terms = input(\"Enter search queries (comma-separated): \").split(\",\")\n",
    "search_year = int(input(\"Enter year to search from: \"))\n",
    "\n",
    "# prompt user for filename\n",
    "filename = input(\"Enter title for scraped data set: \")\n",
    "\n",
    "# set up list to store cleaned tweets\n",
    "tweets_list = []\n",
    "\n",
    "# use snscrape to search for tweets\n",
    "max_tweets = 2000  # limit to 2000 tweets\n",
    "print(\"Searching for tweets...\")\n",
    "try:\n",
    "    for search_term in search_terms:\n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search_term} since:{search_year}-01-01 until:{search_year}-12-31').get_items()):\n",
    "            if i >= max_tweets:\n",
    "                break\n",
    "            # clean the tweet content\n",
    "            content = re.sub(r\"http\\S+\", \"\", tweet.content) # remove URLs\n",
    "            content = re.sub(r\"RT\\s@\\S+\", \"\", content) # remove retweets\n",
    "            content = re.sub(r\"#\\S+\", \"\", content) # remove hashtags\n",
    "            content = re.sub(r\"@\\S+\", \"\", content) # remove mentions\n",
    "            content = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", content) # remove special characters\n",
    "            content = content.strip() # remove leading/trailing white space\n",
    "\n",
    "            # get user location\n",
    "            location = tweet.user.location if tweet.user.location else \"Unknown\"\n",
    "\n",
    "            # get user follower count\n",
    "            followers = tweet.user.followersCount if tweet.user.followersCount else 0\n",
    "\n",
    "            # get user following count\n",
    "            following = tweet.user.friendsCount if tweet.user.friendsCount else 0\n",
    "\n",
    "            # check if user is verified\n",
    "            verified = tweet.user.verified\n",
    "\n",
    "            # get the source of the tweet\n",
    "            tweets_source = tweet.sourceLabel if tweet.sourceLabel else \"Unknown\"\n",
    "\n",
    "            tweets_list.append([tweet.date, tweet.id, content, tweet.url, tweet.user.username, location, followers, following, verified, tweets_source, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit()\n",
    "\n",
    "# save the results to a CSV file\n",
    "with open(f\"{filename}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Date', 'ID', 'Content', 'URL', 'Username', 'Location', 'Followers', 'Following', 'Verified', 'Source', 'Likes', 'Retweets', 'Replies'])  # header row\n",
    "    writer.writerows(tweets_list)\n",
    "\n",
    "# print the first 10 tweets\n",
    "print(\"\\nHere are the first 10 tweets:\")\n",
    "for tweet in tweets_list[:10]:\n",
    "    print(f'{tweet[0]} - {tweet[1]} - {tweet[2]} - {tweet[3]} - {tweet[4]} - {tweet[5]} - {tweet[6]} followers - {tweet[7]} following - Verified: {tweet[8]}, Source: {tweet[9]}, {tweet[10]} likes - {tweet[11]} retweets - {tweet[12]} replies')\n",
    "\n",
    "print(f\"\\nThank you for using my tool! Open {filename}.csv to view your scraped data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac328fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search queries (comma-separated): O2\n",
      "Enter year to search from: 2017\n",
      "Enter title for scraped data set: O2_2017_tweets\n",
      "Searching for tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED MUHAMMED\\AppData\\Local\\Temp\\ipykernel_13676\\3390673566.py:24: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  content = re.sub(r\"http\\S+\", \"\", tweet.content) # remove URLs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the first 10 tweets:\n",
      "2017-12-30 23:58:24+00:00 - 947255583123607552 - If you went to the hospital I assume you were crashing  despite your emergency meds your attack was getting worse amp O2 level was falling You did the right thing But the goal is to prevent things getting to that stage Theres no cure for asthma but most can control it - https://twitter.com/DebbieAlvesDC/status/947255583123607552 - DebbieAlvesDC - Washington, DC - 133 followers - 641 following - Verified: False, Source: Twitter for iPhone, 1 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:58:18+00:00 - 947255557056016385 -  - https://twitter.com/Rsm_a19/status/947255557056016385 - Rsm_a19 - Unknown - 1755 followers - 719 following - Verified: False, Source: Twitter for iPhone, 0 likes - 0 retweets - 1 replies\n",
      "2017-12-30 23:58:14+00:00 - 947255539049926656 -  - https://twitter.com/O2_Aircos/status/947255539049926656 - O2_Aircos - So this is how liberty dies. - 1218 followers - 915 following - Verified: False, Source: Twitter for Android, 0 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:57:07+00:00 - 947255257385418753 - current mood is matty saying i need some more wine before playing fallingforyou at the O2 - https://twitter.com/outthehollows/status/947255257385418753 - outthehollows - Republic of the Philippines - 258 followers - 214 following - Verified: False, Source: Twitter for iPhone, 0 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:56:48+00:00 - 947255178285146113 -  - https://twitter.com/O2_Aircos/status/947255178285146113 - O2_Aircos - So this is how liberty dies. - 1218 followers - 915 following - Verified: False, Source: Twitter for Android, 0 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:55:56+00:00 - 947254960005230592 - As gutted as Im not going to watch him quit again this time at the O2 I think you did the right thing Edward - https://twitter.com/MrZampa7/status/947254960005230592 - MrZampa7 - From Down Souuuuthhhh!! - 581 followers - 863 following - Verified: False, Source: Twitter for iPhone, 1 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:55:46+00:00 - 947254919697981441 - Smashing the place up at Wards birthday at   26 December   O2 - https://twitter.com/WandW_Holland/status/947254919697981441 - WandW_Holland - Amersfoort, The Netherlands - 362 followers - 43 following - Verified: False, Source: Instagram, 1 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:55:17+00:00 - 947254798033739776 -  - https://twitter.com/Holly_O2/status/947254798033739776 - Holly_O2 - 矛盾都市 - 224 followers - 379 following - Verified: False, Source: Twitter for Android, 0 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:54:34+00:00 - 947254617901092865 - Wuthering Heights although Running Up That Hill has to be a close 2nd Loved seeing you this year with Cline Dion both at the O2 and First Direct Arena Leeds Xx - https://twitter.com/adele_dews/status/947254617901092865 - adele_dews - York, England - 24 followers - 85 following - Verified: False, Source: Twitter for iPhone, 0 likes - 0 retweets - 0 replies\n",
      "2017-12-30 23:54:24+00:00 - 947254576511512576 -  - https://twitter.com/LAI_O2/status/947254576511512576 - LAI_O2 - Unknown - 0 followers - 0 following - Verified: False, Source: Twitter for iPhone, 1 likes - 0 retweets - 0 replies\n",
      "\n",
      "Thank you for using my tool! Open O2_2017_tweets.csv to view your scraped data.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import re\n",
    "\n",
    "# prompt user for search queries and year to search from\n",
    "search_terms = input(\"Enter search queries (comma-separated): \").split(\",\")\n",
    "search_year = int(input(\"Enter year to search from: \"))\n",
    "\n",
    "# prompt user for filename\n",
    "filename = input(\"Enter title for scraped data set: \")\n",
    "\n",
    "# set up list to store cleaned tweets\n",
    "tweets_list = []\n",
    "\n",
    "# use snscrape to search for tweets\n",
    "max_tweets = 2000  # limit to 2000 tweets\n",
    "print(\"Searching for tweets...\")\n",
    "try:\n",
    "    for search_term in search_terms:\n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search_term} since:{search_year}-01-01 until:{search_year}-12-31').get_items()):\n",
    "            if i >= max_tweets:\n",
    "                break\n",
    "            # clean the tweet content\n",
    "            content = re.sub(r\"http\\S+\", \"\", tweet.content) # remove URLs\n",
    "            content = re.sub(r\"RT\\s@\\S+\", \"\", content) # remove retweets\n",
    "            content = re.sub(r\"#\\S+\", \"\", content) # remove hashtags\n",
    "            content = re.sub(r\"@\\S+\", \"\", content) # remove mentions\n",
    "            content = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", content) # remove special characters\n",
    "            content = content.strip() # remove leading/trailing white space\n",
    "\n",
    "            # get user location\n",
    "            location = tweet.user.location if tweet.user.location else \"Unknown\"\n",
    "\n",
    "            # get user follower count\n",
    "            followers = tweet.user.followersCount if tweet.user.followersCount else 0\n",
    "\n",
    "            # get user following count\n",
    "            following = tweet.user.friendsCount if tweet.user.friendsCount else 0\n",
    "\n",
    "            # check if user is verified\n",
    "            verified = tweet.user.verified\n",
    "\n",
    "            # get the source of the tweet\n",
    "            tweets_source = tweet.sourceLabel if tweet.sourceLabel else \"Unknown\"\n",
    "\n",
    "            tweets_list.append([tweet.date, tweet.id, content, tweet.url, tweet.user.username, location, followers, following, verified, tweets_source, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit()\n",
    "\n",
    "# save the results to a CSV file\n",
    "with open(f\"{filename}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Date', 'ID', 'Content', 'URL', 'Username', 'Location', 'Followers', 'Following', 'Verified', 'Source', 'Likes', 'Retweets', 'Replies'])  # header row\n",
    "    writer.writerows(tweets_list)\n",
    "\n",
    "# print the first 10 tweets\n",
    "print(\"\\nHere are the first 10 tweets:\")\n",
    "for tweet in tweets_list[:10]:\n",
    "    print(f'{tweet[0]} - {tweet[1]} - {tweet[2]} - {tweet[3]} - {tweet[4]} - {tweet[5]} - {tweet[6]} followers - {tweet[7]} following - Verified: {tweet[8]}, Source: {tweet[9]}, {tweet[10]} likes - {tweet[11]} retweets - {tweet[12]} replies')\n",
    "\n",
    "print(f\"\\nThank you for using my tool! Open {filename}.csv to view your scraped data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd0c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
